<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>AIToolbox: AIToolbox::FactoredMDP::LLR Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">AIToolbox
   </div>
   <div id="projectbrief">A library that offers tools for AI problem solving.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespaceAIToolbox.html">AIToolbox</a></li><li class="navelem"><a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html">FactoredMDP</a></li><li class="navelem"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1LLR.html">LLR</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="classAIToolbox_1_1FactoredMDP_1_1LLR-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">AIToolbox::FactoredMDP::LLR Class Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>This class represents the Learning with Linear Rewards algorithm.  
 <a href="classAIToolbox_1_1FactoredMDP_1_1LLR.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="LLR_8hpp_source.html">LLR.hpp</a>&gt;</code></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a6e5e10f6e96fd5dd9e80af22985ef6ba"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1LLR.html#a6e5e10f6e96fd5dd9e80af22985ef6ba">LLR</a> (<a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#aaa315147132241ce3f4f5f979ceb1b48">Action</a> a, const std::vector&lt; <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#ad94a79edff536157ad47d800547f44d3">Factors</a> &gt; &amp;dependencies)</td></tr>
<tr class="memdesc:a6e5e10f6e96fd5dd9e80af22985ef6ba"><td class="mdescLeft">&#160;</td><td class="mdescRight">Basic constructor.  <a href="#a6e5e10f6e96fd5dd9e80af22985ef6ba">More...</a><br /></td></tr>
<tr class="separator:a6e5e10f6e96fd5dd9e80af22985ef6ba"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a438c27bd84024da40c16f55e7e930162"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#aaa315147132241ce3f4f5f979ceb1b48">Action</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1LLR.html#a438c27bd84024da40c16f55e7e930162">stepUpdateQ</a> (const <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#aaa315147132241ce3f4f5f979ceb1b48">Action</a> &amp;a, const <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#a8f7704ae7054a420b9fc023c76c445c6">Rewards</a> &amp;r)</td></tr>
<tr class="memdesc:a438c27bd84024da40c16f55e7e930162"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function updates the learning process from the previous action and reward.  <a href="#a438c27bd84024da40c16f55e7e930162">More...</a><br /></td></tr>
<tr class="separator:a438c27bd84024da40c16f55e7e930162"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae022d9d35322e8075c67a7c8ac7da3ff"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1FactoredContainer.html">FactoredContainer</a>&lt; <a class="el" href="structAIToolbox_1_1FactoredMDP_1_1QFunctionRule.html">QFunctionRule</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1LLR.html#ae022d9d35322e8075c67a7c8ac7da3ff">getQFunctionRules</a> () const</td></tr>
<tr class="memdesc:ae022d9d35322e8075c67a7c8ac7da3ff"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function obtains the optimal QFunctionRules computed so far.  <a href="#ae022d9d35322e8075c67a7c8ac7da3ff">More...</a><br /></td></tr>
<tr class="separator:ae022d9d35322e8075c67a7c8ac7da3ff"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>This class represents the Learning with Linear Rewards algorithm. </p>
<p>The <a class="el" href="classAIToolbox_1_1FactoredMDP_1_1LLR.html" title="This class represents the Learning with Linear Rewards algorithm. ">LLR</a> algorithm is used on multi-armed bandits, where multiple actions can be taken at the same time.</p>
<p>This algorithm, as described in the paper, is extremely flexible as it both allows multiple actions to be taken at each timestep, while also leaving space for any algorithm which is able to solve the action maximization selection problem. This is possible since the action space can be arbitrarily restricted.</p>
<p>This means that creating an actual generic algorithm out of the paper is pretty hard as it would have to be able to be passed any algorithm and use it. We chose not to do it here.</p>
<p>Here we implement a simple version where a single, factored action is allowed, and we use VE to solve the action selection problem. This pretty much results in simply solving VE with UCB1 weights, together with some learning. </p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a6e5e10f6e96fd5dd9e80af22985ef6ba"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6e5e10f6e96fd5dd9e80af22985ef6ba">&#9670;&nbsp;</a></span>LLR()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">AIToolbox::FactoredMDP::LLR::LLR </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#aaa315147132241ce3f4f5f979ceb1b48">Action</a>&#160;</td>
          <td class="paramname"><em>a</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#ad94a79edff536157ad47d800547f44d3">Factors</a> &gt; &amp;&#160;</td>
          <td class="paramname"><em>dependencies</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Basic constructor. </p>
<p>In order to keep track of each partial action's averages and counts, we need to know which factors are actually dependent on each other.</p>
<p>So suppose we have a three-factored action space {1,2,3}, and two local reward functions using factors {0,1}, and {1,2}. Then {{0,1}, {1,2}} is going to be the dependency parameter.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">a</td><td>The action space. </td></tr>
    <tr><td class="paramname">dependencies</td><td>The dependencies in the problem. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="ae022d9d35322e8075c67a7c8ac7da3ff"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae022d9d35322e8075c67a7c8ac7da3ff">&#9670;&nbsp;</a></span>getQFunctionRules()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1FactoredContainer.html">FactoredContainer</a>&lt;<a class="el" href="structAIToolbox_1_1FactoredMDP_1_1QFunctionRule.html">QFunctionRule</a>&gt; AIToolbox::FactoredMDP::LLR::getQFunctionRules </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function obtains the optimal QFunctionRules computed so far. </p>
<p>These rules skip the exploration part, to allow the creation of a policy using the learned QFunction (since otherwise this algorithm would forever explore).</p>
<p>Note that this function must perform a complete copy of all internal rules, as those contain the exploration factors of UCB1 baked in.</p>
<dl class="section return"><dt>Returns</dt><dd>The learned optimal QFunctionRules. </dd></dl>

</div>
</div>
<a id="a438c27bd84024da40c16f55e7e930162"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a438c27bd84024da40c16f55e7e930162">&#9670;&nbsp;</a></span>stepUpdateQ()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#aaa315147132241ce3f4f5f979ceb1b48">Action</a> AIToolbox::FactoredMDP::LLR::stepUpdateQ </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#aaa315147132241ce3f4f5f979ceb1b48">Action</a> &amp;&#160;</td>
          <td class="paramname"><em>a</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#a8f7704ae7054a420b9fc023c76c445c6">Rewards</a> &amp;&#160;</td>
          <td class="paramname"><em>r</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function updates the learning process from the previous action and reward. </p>
<p>Note that the rewards parameter is going to have as many elements as the number of local payoff functions passed as input in the constructor.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">a</td><td>The action taken in the previous step. </td></tr>
    <tr><td class="paramname">r</td><td>The rewards obtained in the previous step.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The optimal action to take at the next timestep. </dd></dl>

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>include/AIToolbox/FactoredMDP/Algorithms/<a class="el" href="LLR_8hpp_source.html">LLR.hpp</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Thu Jun 8 2017 15:04:19 for AIToolbox by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
