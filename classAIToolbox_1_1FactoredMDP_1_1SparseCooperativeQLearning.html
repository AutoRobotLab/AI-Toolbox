<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>AIToolbox: AIToolbox::FactoredMDP::SparseCooperativeQLearning Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">AIToolbox
   </div>
   <div id="projectbrief">A library that offers tools for AI problem solving.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Class&#160;List</span></a></li>
      <li><a href="classes.html"><span>Class&#160;Index</span></a></li>
      <li><a href="hierarchy.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&#160;Members</span></a></li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespaceAIToolbox.html">AIToolbox</a></li><li class="navelem"><a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html">FactoredMDP</a></li><li class="navelem"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html">SparseCooperativeQLearning</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">AIToolbox::FactoredMDP::SparseCooperativeQLearning Class Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>This class represents the Sparse Cooperative QLearning algorithm.  
 <a href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="SparseCooperativeQLearning_8hpp_source.html">SparseCooperativeQLearning.hpp</a>&gt;</code></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a5b1f4d2a05ac4d313ae63c552ab83de8"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html#a5b1f4d2a05ac4d313ae63c552ab83de8">SparseCooperativeQLearning</a> (<a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#a04a63a02e9bcfe84b74a358f4b0af350">State</a> S, <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#aaa315147132241ce3f4f5f979ceb1b48">Action</a> A, double discount, double alpha)</td></tr>
<tr class="memdesc:a5b1f4d2a05ac4d313ae63c552ab83de8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Basic constructor.  <a href="#a5b1f4d2a05ac4d313ae63c552ab83de8">More...</a><br /></td></tr>
<tr class="separator:a5b1f4d2a05ac4d313ae63c552ab83de8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa2d5011adb8c04a2209261d433f3911a"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html#aa2d5011adb8c04a2209261d433f3911a">reserveRules</a> (size_t s)</td></tr>
<tr class="memdesc:aa2d5011adb8c04a2209261d433f3911a"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function reserves memory for at least s rules.  <a href="#aa2d5011adb8c04a2209261d433f3911a">More...</a><br /></td></tr>
<tr class="separator:aa2d5011adb8c04a2209261d433f3911a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3a5c89f1f030ae7704c743a0b70c5150"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html#a3a5c89f1f030ae7704c743a0b70c5150">insertRule</a> (<a class="el" href="structAIToolbox_1_1FactoredMDP_1_1QFunctionRule.html">QFunctionRule</a> rule)</td></tr>
<tr class="memdesc:a3a5c89f1f030ae7704c743a0b70c5150"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function inserts a <a class="el" href="structAIToolbox_1_1FactoredMDP_1_1QFunctionRule.html" title="This struct represents a single state/action/value tuple. ">QFunctionRule</a> in the covered set.  <a href="#a3a5c89f1f030ae7704c743a0b70c5150">More...</a><br /></td></tr>
<tr class="separator:a3a5c89f1f030ae7704c743a0b70c5150"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2d9567bc933766880ed32df8ec11cd11"><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html#a2d9567bc933766880ed32df8ec11cd11">rulesSize</a> () const </td></tr>
<tr class="memdesc:a2d9567bc933766880ed32df8ec11cd11"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function returns the number of rules currently stored.  <a href="#a2d9567bc933766880ed32df8ec11cd11">More...</a><br /></td></tr>
<tr class="separator:a2d9567bc933766880ed32df8ec11cd11"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8b99847e83c8c78ec4e3faab7cbd1a4f"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html#a8b99847e83c8c78ec4e3faab7cbd1a4f">setLearningRate</a> (double a)</td></tr>
<tr class="memdesc:a8b99847e83c8c78ec4e3faab7cbd1a4f"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function sets the learning rate parameter.  <a href="#a8b99847e83c8c78ec4e3faab7cbd1a4f">More...</a><br /></td></tr>
<tr class="separator:a8b99847e83c8c78ec4e3faab7cbd1a4f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acca88d699e558c981261bf8687a1ff3c"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html#acca88d699e558c981261bf8687a1ff3c">getLearningRate</a> () const </td></tr>
<tr class="memdesc:acca88d699e558c981261bf8687a1ff3c"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function will return the current set learning rate parameter.  <a href="#acca88d699e558c981261bf8687a1ff3c">More...</a><br /></td></tr>
<tr class="separator:acca88d699e558c981261bf8687a1ff3c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a98713d76e241159c4342d26f850e9309"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html#a98713d76e241159c4342d26f850e9309">setDiscount</a> (double d)</td></tr>
<tr class="memdesc:a98713d76e241159c4342d26f850e9309"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function sets the new discount parameter.  <a href="#a98713d76e241159c4342d26f850e9309">More...</a><br /></td></tr>
<tr class="separator:a98713d76e241159c4342d26f850e9309"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac34ec205dee5e55a66fc72adc2e99d09"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html#ac34ec205dee5e55a66fc72adc2e99d09">getDiscount</a> () const </td></tr>
<tr class="memdesc:ac34ec205dee5e55a66fc72adc2e99d09"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function returns the currently set discount parameter.  <a href="#ac34ec205dee5e55a66fc72adc2e99d09">More...</a><br /></td></tr>
<tr class="separator:ac34ec205dee5e55a66fc72adc2e99d09"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a18b77f8fc810fe2c906b0aad81971fde"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#aaa315147132241ce3f4f5f979ceb1b48">Action</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html#a18b77f8fc810fe2c906b0aad81971fde">stepUpdateQ</a> (const <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#a04a63a02e9bcfe84b74a358f4b0af350">State</a> &amp;s, const <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#aaa315147132241ce3f4f5f979ceb1b48">Action</a> &amp;a, const <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#a04a63a02e9bcfe84b74a358f4b0af350">State</a> &amp;s1, const <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#a8f7704ae7054a420b9fc023c76c445c6">Rewards</a> &amp;rew)</td></tr>
<tr class="memdesc:a18b77f8fc810fe2c906b0aad81971fde"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function updates the internal QFunctionRules based on experience.  <a href="#a18b77f8fc810fe2c906b0aad81971fde">More...</a><br /></td></tr>
<tr class="separator:a18b77f8fc810fe2c906b0aad81971fde"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abe06d1bda4db1d3e64215d0c9a38c132"><td class="memItemLeft" align="right" valign="top">const <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#a04a63a02e9bcfe84b74a358f4b0af350">State</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html#abe06d1bda4db1d3e64215d0c9a38c132">getS</a> () const </td></tr>
<tr class="memdesc:abe06d1bda4db1d3e64215d0c9a38c132"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function returns the state space on which <a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html" title="This class represents the Sparse Cooperative QLearning algorithm. ">SparseCooperativeQLearning</a> is working.  <a href="#abe06d1bda4db1d3e64215d0c9a38c132">More...</a><br /></td></tr>
<tr class="separator:abe06d1bda4db1d3e64215d0c9a38c132"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6a4ae0a4155d66bb5adfbc79f4d2dd9e"><td class="memItemLeft" align="right" valign="top">const <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#aaa315147132241ce3f4f5f979ceb1b48">Action</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html#a6a4ae0a4155d66bb5adfbc79f4d2dd9e">getA</a> () const </td></tr>
<tr class="memdesc:a6a4ae0a4155d66bb5adfbc79f4d2dd9e"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function returns the action space on which <a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html" title="This class represents the Sparse Cooperative QLearning algorithm. ">SparseCooperativeQLearning</a> is working.  <a href="#a6a4ae0a4155d66bb5adfbc79f4d2dd9e">More...</a><br /></td></tr>
<tr class="separator:a6a4ae0a4155d66bb5adfbc79f4d2dd9e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3e863dd163abf0c588a236e60e2b5a14"><td class="memItemLeft" align="right" valign="top">const <a class="el" href="classAIToolbox_1_1FactoredMDP_1_1FactoredContainer.html">FactoredContainer</a>&lt; <a class="el" href="structAIToolbox_1_1FactoredMDP_1_1QFunctionRule.html">QFunctionRule</a> &gt; &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html#a3e863dd163abf0c588a236e60e2b5a14">getQFunctionRules</a> () const </td></tr>
<tr class="memdesc:a3e863dd163abf0c588a236e60e2b5a14"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function returns a reference to the internal <a class="el" href="classAIToolbox_1_1FactoredMDP_1_1FactoredContainer.html" title="This class is a container which uses PartialFactors as keys. ">FactoredContainer</a> of QFunctionRules.  <a href="#a3e863dd163abf0c588a236e60e2b5a14">More...</a><br /></td></tr>
<tr class="separator:a3e863dd163abf0c588a236e60e2b5a14"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>This class represents the Sparse Cooperative QLearning algorithm. </p>
<p>This algorithm is designed to work in cooperative multi-agent problems, but can as easily be used for factored state/action single agent MDPs (since the two things are equivalent).</p>
<p>Rather than having a single huge QFunction covering all possible state/action pairs, <a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html" title="This class represents the Sparse Cooperative QLearning algorithm. ">SparseCooperativeQLearning</a> keeps its QFunction split into <a class="el" href="structAIToolbox_1_1FactoredMDP_1_1QFunctionRule.html" title="This struct represents a single state/action/value tuple. ">QFunctionRule</a>. Each rule covers a specific reward that can be obtained via a PartialState and PartialAction.</p>
<p>As the agent interacts with the world, these rules are updated to better reflect the rewards obtained from the environment. At each timestep, each rule applicable on the starting State and Action are updated based on the next State and the optimal Action that is computed with the existing rules via <a class="el" href="classAIToolbox_1_1FactoredMDP_1_1VariableElimination.html" title="This class represents the Variable Elimination process. ">VariableElimination</a>.</p>
<p>Aside from this, this algorithm is very similar to the single agent <a class="el" href="classAIToolbox_1_1MDP_1_1QLearning.html" title="This class represents the QLearning algorithm. ">MDP::QLearning</a> (hence the name). </p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a class="anchor" id="a5b1f4d2a05ac4d313ae63c552ab83de8"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">AIToolbox::FactoredMDP::SparseCooperativeQLearning::SparseCooperativeQLearning </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#a04a63a02e9bcfe84b74a358f4b0af350">State</a>&#160;</td>
          <td class="paramname"><em>S</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#aaa315147132241ce3f4f5f979ceb1b48">Action</a>&#160;</td>
          <td class="paramname"><em>A</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>discount</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>alpha</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Basic constructor. </p>
<p>This constructor initializes all data structures and parameters for the correct functioning of QLearning.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">S</td><td>The factored state space of the environment. </td></tr>
    <tr><td class="paramname">A</td><td>The factored action space for the agent. </td></tr>
    <tr><td class="paramname">discount</td><td>The discount for future rewards. </td></tr>
    <tr><td class="paramname">alpha</td><td>The learning parameter. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a class="anchor" id="a6a4ae0a4155d66bb5adfbc79f4d2dd9e"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">const <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#aaa315147132241ce3f4f5f979ceb1b48">Action</a>&amp; AIToolbox::FactoredMDP::SparseCooperativeQLearning::getA </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function returns the action space on which <a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html" title="This class represents the Sparse Cooperative QLearning algorithm. ">SparseCooperativeQLearning</a> is working. </p>
<dl class="section return"><dt>Returns</dt><dd>The number of actions. </dd></dl>

</div>
</div>
<a class="anchor" id="ac34ec205dee5e55a66fc72adc2e99d09"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double AIToolbox::FactoredMDP::SparseCooperativeQLearning::getDiscount </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function returns the currently set discount parameter. </p>
<dl class="section return"><dt>Returns</dt><dd>The currently set discount parameter. </dd></dl>

</div>
</div>
<a class="anchor" id="acca88d699e558c981261bf8687a1ff3c"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double AIToolbox::FactoredMDP::SparseCooperativeQLearning::getLearningRate </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function will return the current set learning rate parameter. </p>
<dl class="section return"><dt>Returns</dt><dd>The currently set learning rate parameter. </dd></dl>

</div>
</div>
<a class="anchor" id="a3e863dd163abf0c588a236e60e2b5a14"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">const <a class="el" href="classAIToolbox_1_1FactoredMDP_1_1FactoredContainer.html">FactoredContainer</a>&lt;<a class="el" href="structAIToolbox_1_1FactoredMDP_1_1QFunctionRule.html">QFunctionRule</a>&gt;&amp; AIToolbox::FactoredMDP::SparseCooperativeQLearning::getQFunctionRules </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function returns a reference to the internal <a class="el" href="classAIToolbox_1_1FactoredMDP_1_1FactoredContainer.html" title="This class is a container which uses PartialFactors as keys. ">FactoredContainer</a> of QFunctionRules. </p>
<dl class="section return"><dt>Returns</dt><dd>The internal QFunctionRules. </dd></dl>

</div>
</div>
<a class="anchor" id="abe06d1bda4db1d3e64215d0c9a38c132"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">const <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#a04a63a02e9bcfe84b74a358f4b0af350">State</a>&amp; AIToolbox::FactoredMDP::SparseCooperativeQLearning::getS </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function returns the state space on which <a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html" title="This class represents the Sparse Cooperative QLearning algorithm. ">SparseCooperativeQLearning</a> is working. </p>
<dl class="section return"><dt>Returns</dt><dd>The number of states. </dd></dl>

</div>
</div>
<a class="anchor" id="a3a5c89f1f030ae7704c743a0b70c5150"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void AIToolbox::FactoredMDP::SparseCooperativeQLearning::insertRule </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="structAIToolbox_1_1FactoredMDP_1_1QFunctionRule.html">QFunctionRule</a>&#160;</td>
          <td class="paramname"><em>rule</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function inserts a <a class="el" href="structAIToolbox_1_1FactoredMDP_1_1QFunctionRule.html" title="This struct represents a single state/action/value tuple. ">QFunctionRule</a> in the covered set. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">rule</td><td>The new rule to cover. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="aa2d5011adb8c04a2209261d433f3911a"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void AIToolbox::FactoredMDP::SparseCooperativeQLearning::reserveRules </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>s</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function reserves memory for at least s rules. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">s</td><td>The number of rules to be reserved. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a2d9567bc933766880ed32df8ec11cd11"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">size_t AIToolbox::FactoredMDP::SparseCooperativeQLearning::rulesSize </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function returns the number of rules currently stored. </p>
<dl class="section return"><dt>Returns</dt><dd>The number of stored QFunctionRules. </dd></dl>

</div>
</div>
<a class="anchor" id="a98713d76e241159c4342d26f850e9309"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void AIToolbox::FactoredMDP::SparseCooperativeQLearning::setDiscount </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>d</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function sets the new discount parameter. </p>
<p>The discount parameter controls the amount that future rewards are considered by <a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html" title="This class represents the Sparse Cooperative QLearning algorithm. ">SparseCooperativeQLearning</a>. If 1, then any reward is the same, if obtained now or in a million timesteps. Thus the algorithm will optimize overall reward accretion. When less than 1, rewards obtained in the presents are valued more than future rewards.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">d</td><td>The new discount factor. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a8b99847e83c8c78ec4e3faab7cbd1a4f"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void AIToolbox::FactoredMDP::SparseCooperativeQLearning::setLearningRate </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>a</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function sets the learning rate parameter. </p>
<p>The learning parameter determines the speed at which the QFunctions are modified with respect to new data. In fully deterministic environments (such as an agent moving through a grid, for example), this parameter can be safely set to 1.0 for maximum learning.</p>
<p>On the other side, in stochastic environments, in order to converge this parameter should be higher when first starting to learn, and decrease slowly over time.</p>
<p>Otherwise it can be kept somewhat high if the environment dynamics change progressively, and the algorithm will adapt accordingly. The final behavior of <a class="el" href="classAIToolbox_1_1FactoredMDP_1_1SparseCooperativeQLearning.html" title="This class represents the Sparse Cooperative QLearning algorithm. ">SparseCooperativeQLearning</a> is very dependent on this parameter.</p>
<p>The learning rate parameter must be &gt; 0.0 and &lt;= 1.0, otherwise the function will throw an std::invalid_argument.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">a</td><td>The new learning rate parameter. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a18b77f8fc810fe2c906b0aad81971fde"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#aaa315147132241ce3f4f5f979ceb1b48">Action</a> AIToolbox::FactoredMDP::SparseCooperativeQLearning::stepUpdateQ </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#a04a63a02e9bcfe84b74a358f4b0af350">State</a> &amp;&#160;</td>
          <td class="paramname"><em>s</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#aaa315147132241ce3f4f5f979ceb1b48">Action</a> &amp;&#160;</td>
          <td class="paramname"><em>a</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#a04a63a02e9bcfe84b74a358f4b0af350">State</a> &amp;&#160;</td>
          <td class="paramname"><em>s1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespaceAIToolbox_1_1FactoredMDP.html#a8f7704ae7054a420b9fc023c76c445c6">Rewards</a> &amp;&#160;</td>
          <td class="paramname"><em>rew</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function updates the internal QFunctionRules based on experience. </p>
<p>This function takes a single experience point and uses it to update the QFunctionRules. Since in order to do this we have to compute the best possible action for the next timestep, we return it in case it is needed.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">s</td><td>The previous state. </td></tr>
    <tr><td class="paramname">a</td><td>The action performed. </td></tr>
    <tr><td class="paramname">s1</td><td>The new state. </td></tr>
    <tr><td class="paramname">rew</td><td>The reward obtained.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The best action to be performed in the next timestep. </dd></dl>

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>include/AIToolbox/FactoredMDP/Algorithms/<a class="el" href="SparseCooperativeQLearning_8hpp_source.html">SparseCooperativeQLearning.hpp</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Wed Jan 4 2017 15:14:44 for AIToolbox by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
</body>
</html>
